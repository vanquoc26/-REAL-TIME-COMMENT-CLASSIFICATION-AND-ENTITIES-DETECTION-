{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = \"C:\\\\hadoop\"\n",
    "os.environ['PATH'] += \";C:\\\\hadoop\\\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://HQM-CDN:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigData</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20726cb00d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala_version = '2.12'\n",
    "spark_version = '3.5.1'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.3.1'\n",
    "]\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"BigData\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"16\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "conf=SparkConf()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'doanBD_3'\n",
    "kafka_server = 'localhost:9092'\n",
    "streamRawDf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic_name).option(\"startingOffsets\",\"latest\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the saved model components\n",
    "dct_objects = joblib.load('model/all_objects.pkl')\n",
    "dct_model = joblib.load('model/multi_target_lr_model.pkl')\n",
    "dct_vectorizer = joblib.load('model/vectorizer.pkl')\n",
    "\n",
    "clf_model = joblib.load('model/random_forest_model.pkl')\n",
    "clf_vectorizer = joblib.load('model/rf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict entities in a comment\n",
    "def predict_entities(comment, model, vectorizer, all_objects):\n",
    "    X_new = vectorizer.transform([comment])\n",
    "    Y_new_pred = model.predict(X_new)\n",
    "    entities = [all_objects[i] for i in range(len(all_objects)) if Y_new_pred[0, i] == 1]\n",
    "    return ', '.join(entities)\n",
    "\n",
    "def classify_comment(comment, model, vectorizer):    \n",
    "    X_new = vectorizer.transform([comment])\n",
    "    Y_new_pred = model.predict(X_new)\n",
    "    predict = int(Y_new_pred[0])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = '[hay qua]'\n",
    "classify_comment(comment, clf_model, clf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDF\n",
    "predict_entities_udf = udf(lambda comment: predict_entities(comment, dct_model, dct_vectorizer, dct_objects), StringType())\n",
    "classify_comment_udf = udf(lambda comment: classify_comment(comment, clf_model, clf_vectorizer), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_json_column(col):\n",
    "    return f.udf(lambda x: json.loads(x), StringType())(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = streamRawDf.selectExpr(\"CAST(value AS STRING) as value\").withColumn(\"Comment\", decode_json_column(\"value\"))\n",
    "\n",
    "stream_writer = (df.writeStream.trigger(processingTime=\"5 seconds\").outputMode(\"append\").format(\"memory\").queryName(\"Table\"))\n",
    "query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing live view refreshed every 5 seconds\n",
      "Seconds passed: 20\n",
      "+----------------------------------------------------------------------------------------------------+--------+----------+\n",
      "|                                                                                             Comment|Entities|Classified|\n",
      "+----------------------------------------------------------------------------------------------------+--------+----------+\n",
      "|                                                                                [kết mỗi di dimaria]| dimaria|         1|\n",
      "|                                                                 [chân ricardo quaresma dẻo vãi cặc]|        |         1|\n",
      "|                                    [hầu như những người thành công nhất đều xuất thân từ nghèo khổ]|        |         1|\n",
      "|                                                                     [quang hai se tim lai phong độ]|        |         0|\n",
      "|                                                              [chưa thấy nhà đài nào quay quan chức]|        |         0|\n",
      "|                                                                   [kante xuất thân siêu nghèo luôn]|   kante|         2|\n",
      "|                                                                                          [thật chứ]|        |         0|\n",
      "|                                                                                   [garnacho đẹp ác]|garnacho|         1|\n",
      "|                                                                                             [tuyệt]|        |         1|\n",
      "|[và chính các cầu thủ xuất thân nghèo khó đó đã vươn mình trở thành những hàng đầu thế giới, làm ...|        |         0|\n",
      "|                                                           [những người nghèo khổ da thương đen xạm]|        |         2|\n",
      "+----------------------------------------------------------------------------------------------------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark_3.5.1\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark_3.5.1\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming query interrupted.\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "x = 0\n",
    "while(True):\n",
    "    try:\n",
    "        # Query the in-memory table\n",
    "        result_df = spark.sql(\"SELECT * FROM Table\")\n",
    "        result_df = result_df.withColumn(\"Entities\", predict_entities_udf(f.col(\"Comment\")))\n",
    "        result_df = result_df.withColumn(\"Classified\", classify_comment_udf(f.col(\"Comment\")))\n",
    "        result_df = result_df.select(\"Comment\", \"Entities\", \"Classified\")\n",
    "        all_rows = result_df.collect()\n",
    "\n",
    "        # Lấy 20 dòng cuối cùng\n",
    "        last_20_rows = all_rows[-20:]\n",
    "\n",
    "        # Tạo một DataFrame mới từ 20 dòng cuối cùng\n",
    "        tail_df = spark.createDataFrame(last_20_rows, result_df.schema)\n",
    "\n",
    "        # Display the streaming query status\n",
    "        clear_output(wait=True)\n",
    "        print('Showing live view refreshed every 5 seconds')\n",
    "        print(f'Seconds passed: {x*5}')\n",
    "\n",
    "        # Hiển thị 20 dòng cuối cùng\n",
    "        tail_df.show(truncate=100)\n",
    "\n",
    "        sleep(5)\n",
    "        x += 1  \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Streaming query interrupted.\")\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
